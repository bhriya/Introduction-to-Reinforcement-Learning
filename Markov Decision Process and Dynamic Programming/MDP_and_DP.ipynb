{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Environment\n",
        "Creating the environment in order to get the sequences of actions, rewards and next states."
      ],
      "metadata": {
        "id": "W555JgH1P_mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m5GOg8YJTTOx"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "def weighted_choice(v, p):\n",
        "   total = sum(p)\n",
        "   r = random.uniform(0, total)\n",
        "   upto = 0\n",
        "   for c, w in zip(v,p):\n",
        "      if upto + w >= r:\n",
        "         return c\n",
        "      upto += w\n",
        "   assert False, \"Shouldn't get here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HEjzzIF1TXyI"
      },
      "outputs": [],
      "source": [
        "class MDP_Environment:\n",
        "    def __init__(self, transition_probs, rewards, initial_state='s0'):\n",
        "        self._transition_probs = transition_probs\n",
        "        self._rewards = rewards\n",
        "        self._initial_state = initial_state\n",
        "        self.n_states = len(transition_probs)\n",
        "        self.reset()\n",
        "\n",
        "    def get_all_states(self):\n",
        "        \"\"\" return a tuple of all possible states \"\"\"\n",
        "        return tuple(self._transition_probs.keys())\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
        "        return tuple(self._transition_probs.get(state, {}).keys())\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
        "        return len(self.get_possible_actions(state)) == 0\n",
        "\n",
        "    def get_next_states(self, state, action):\n",
        "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
        "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._transition_probs[state][action]\n",
        "\n",
        "    def get_transition_prob(self, state, action, next_state):\n",
        "        \"\"\" return P(next_state | state, action) \"\"\"\n",
        "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
        "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._rewards.get(state, {}).get(action, {}).get(next_state, -0.01)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" reset the game, return the initial state\"\"\"\n",
        "        if self._initial_state is None:\n",
        "            self._current_state = random.choice(tuple(self._transition_probs.keys()))\n",
        "        elif self._initial_state in self._transition_probs:\n",
        "            self._current_state = self._initial_state\n",
        "        elif callable(self._initial_state):\n",
        "            self._current_state = self._initial_state()\n",
        "        else:\n",
        "            raise ValueError(\"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
        "        return self._current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
        "        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n",
        "        next_state = weighted_choice(possible_states, p=probs)\n",
        "        reward = self.get_reward(self._current_state, action, next_state)\n",
        "        is_done = self.is_terminal(next_state)\n",
        "        self._current_state = next_state\n",
        "        return next_state, reward, is_done, {}\n",
        "\n",
        "    def render(self):\n",
        "        if(self._current_state=='s3'):\n",
        "          print(\"Currently at goal state %s\" % self._current_state)\n",
        "        if(self._current_state=='s7'):\n",
        "          print(\"Currently at hole state %s\" % self._current_state)\n",
        "        if(self._current_state!='s3' and self._current_state!='s7'):\n",
        "          print(\"Currently at %s\" % self._current_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "left = a0\n",
        "\n",
        "up = a1\n",
        "\n",
        "right = a2\n",
        "\n",
        "down = a3"
      ],
      "metadata": {
        "id": "0KfyDtIU_i3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0WxXSZKjT67i"
      },
      "outputs": [],
      "source": [
        "# creating dictionaries for transition probabilities and transition rewards as given in the problem statement\n",
        "transition_probs = {'s0' : {'a0' : {'s0' : 0.9, 's4' : 0.1}, 'a1' : {'s0' : 0.9, 's1' : 0.1}, 'a2' : {'s1' : 0.8, 's0' : 0.1, 's4' : 0.1}, 'a3' : {'s4' : 0.8, 's0' : 0.1, 's1' : 0.1}},\n",
        "                    's1' : {'a0' : {'s1' : 0.2, 's0' : 0.8}, 'a1' : {'s0' : 0.1, 's1' : 0.8, 's2' : 0.1}, 'a2' : {'s2' : 0.8, 's1' : 0.2}, 'a3' : {'s1' : 0.8, 's0' : 0.1, 's2' : 0.1}},\n",
        "                    's2' : {'a0' : {'s1' : 0.8, 's2' : 0.1, 's6' : 0.1}, 'a1' : {'s2' : 0.8, 's1' : 0.1, 's3' : 0.1}, 'a2' : {'s3' : 0.8, 's2' : 0.1, 's6' : 0.1}, 'a3' : {'s6' : 0.8, 's3' : 0.1, 's1' : 0.1}},\n",
        "                    's3' : {},\n",
        "                    's4' : {'a0' : {'s4' : 0.8, 's0' : 0.1, 's8' : 0.1}, 'a1' : {'s0' : 0.8, 's4' : 0.2}, 'a2' : {'s4' : 0.8, 's0' : 0.1, 's8' : 0.1}, 'a3' : {'s8' : 0.8, 's4' : 0.2}},\n",
        "                    's5' : {},\n",
        "                    's6' : {'a0' : {'s6' : 0.8, 's2' : 0.1, 's10' : 0.1}, 'a1' : {'s2' : 0.8, 's6' : 0.1, 's7' : 0.1}, 'a2' : {'s7' : 0.8, 's2' : 0.1, 's10' : 0.1}, 'a3' : {'s10' : 0.8, 's6' : 0.1, 's7' : 0.1}},\n",
        "                    's7' : {},\n",
        "                    's8' : {'a0' : {'s8' : 0.9, 's4' : 0.1}, 'a1' : {'s4' : 0.8, 's8' : 0.1, 's9' : 0.1}, 'a2' : {'s9' : 0.8, 's4' : 0.1, 's8' : 0.1}, 'a3' : {'s8' : 0.9, 's9' : 0.1}},\n",
        "                    's9' : {'a0' : {'s8' : 0.8, 's9' : 0.2}, 'a1' : {'s9' : 0.8, 's8' : 0.1, 's10' : 0.1}, 'a2' :{'s10' : 0.8, 's9' : 0.2}, 'a3' : {'s9' : 0.8, 's10' : 0.1, 's8' : 0.1}},\n",
        "                    's10' : {'a0' : {'s9' : 0.8, 's6' : 0.1, 's10' : 0.1}, 'a1' : {'s6' : 0.8, 's9' : 0.1, 's11' : 0.1}, 'a2' : {'s11' : 0.8, 's6' : 0.1, 's10' : 0.1}, 'a3' : {'s10' : 0.8, 's11' : 0.1, 's9' : 0.1}},\n",
        "                    's11' : {'a0' : {'s10' : 0.8, 's7' : 0.1, 's11' : 0.1}, 'a1' : {'s7' : 0.8, 's10' : 0.1, 's11' : 0.1}, 'a2' : {'s11' : 0.9, 's7' : 0.1}, 'a3' : {'s11' : 0.9, 's10' : 0.1}}}\n",
        "\n",
        "rewards = {'s2' : {'a1' : {'s3' : 1}, 'a2' : {'s3' : 1}, 'a3' : {'s3' : 1}},\n",
        "           's6' : {'a1' : {'s7' : -1}, 'a2' : {'s7' : -1}, 'a3' : {'s7' : -1}},\n",
        "           's11' : {'a0' : {'s7' : -1}, 'a1' : {'s7' : -1}, 'a2' : {'s7' : -1}}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rKLaUXrnzN6L"
      },
      "outputs": [],
      "source": [
        "env = MDP_Environment(transition_probs, rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For both the policy iteration and value iteration algorithms implemented below the value corresponding to state 5 is set to 0 as a default value.\n",
        "While the action corresponding to state 5 as well as terminal state is set to None."
      ],
      "metadata": {
        "id": "dU9XvcnbiRPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration\n",
        "In policy iteration, we start by choosing an arbitrary policy $\\boldsymbol{\\pi}$. Then, we iteratively evaluate and improve the policy until convergence.\n",
        "We evaluate a policy $\\pi(s)$ by calculating the state value function\n",
        "V(s):\n",
        "\n",
        "$V(s) = \\sum_{s',r'}p(s', r|s,\\pi(s))[r+\\gamma V(s')]$\n",
        "\n",
        "Then, we calculate the improved policy by using one-step look-ahead to replace the initial policy $\\pi(s)$:\n",
        "\n",
        "$\\pi(s) = arg\\max_{a} \\sum_{s',r'} p(s', r|s,a)[r+\\gamma V(s')]$"
      ],
      "metadata": {
        "id": "JhOXVXjyn_T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the beginning, we donâ€™t care about the initial policy \\pi_0 being optimal or not. During the execution, we concentrate on improving it on every iteration by repeating policy evaluation and policy improvement steps. Using this algorithm we produce a chain of policies, where each policy is an improvement over the previous one:\n",
        "\n",
        "  $\\pi_0 \\xrightarrow[]{\\text{E}} v_{\\pi_0} \\xrightarrow[]{\\text{I}} \\pi_1 \\xrightarrow[]{\\text{E}} v_{\\pi_1} \\xrightarrow[]{\\text{I}} \\pi_2 \\xrightarrow[]{\\text{E}} \\dotsi \\xrightarrow[]{\\text{I}} \\pi_* \\xrightarrow[]{\\text{E}} v_{*}$"
      ],
      "metadata": {
        "id": "Uplzz1X0CFtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = list(env.get_all_states())\n",
        "# choosing a random action space for each state\n",
        "action_space = []\n",
        "for i in range(len(states)):\n",
        "  actions = env.get_possible_actions(states[i])\n",
        "  if(len(actions)>0):\n",
        "    act = random.choice(actions)\n",
        "  else:\n",
        "    act = 'None'\n",
        "  action_space.append(act)\n",
        "print(action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuHSj0lKhQEd",
        "outputId": "235b6014-3c26-4816-c6e7-e94a8c92edd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a2', 'a0', 'a0', 'None', 'a1', 'None', 'a1', 'None', 'a2', 'a0', 'a0', 'a1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the value function for the current action space to a vector of zeros\n",
        "v_k = []\n",
        "for i in range(len(states)):\n",
        "  v_k.append(0)"
      ],
      "metadata": {
        "id": "i6gCp5KkqrVw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop for policy improvement\n",
        "flag1 = 1\n",
        "num_steps = 0\n",
        "while(flag1==1):\n",
        "  num_steps += 1\n",
        "  #loop for policy evaluation\n",
        "  flag2 = 1\n",
        "  while(flag2==1):\n",
        "    v_kplus1 = []\n",
        "    for i in range(len(states)):\n",
        "      if(action_space[i]!='None'):\n",
        "        next_states = list(env.get_next_states(states[i], action_space[i]).keys())\n",
        "        sum = 0\n",
        "        for j in range(len(next_states)):\n",
        "          sum = env.get_transition_prob(states[i], action_space[i], next_states[j])*(env.get_reward(states[i], action_space[i], next_states[j]) + v_k[states.index(next_states[j])]) + sum\n",
        "        v_kplus1.append(sum)\n",
        "      else:\n",
        "        v_kplus1.append(0)\n",
        "    max = -1\n",
        "    for k in range(len(states)):\n",
        "      if(abs(v_kplus1[k]-v_k[k])>max):\n",
        "        max = abs(v_kplus1[k]-v_k[k])\n",
        "    if(max<=0.01):\n",
        "      flag2 = 0\n",
        "    v_k = v_kplus1\n",
        "  action_space_dash = []\n",
        "  for k in range(len(states)):\n",
        "    if(action_space[k]!='None'):\n",
        "      actions = env.get_possible_actions(states[k])\n",
        "      max = -1\n",
        "      max_index = 0\n",
        "      for act in range(len(actions)):\n",
        "        next_states = list(env.get_next_states(states[k], actions[act]).keys())\n",
        "        sum = 0\n",
        "        for j in range(len(next_states)):\n",
        "          sum = env.get_transition_prob(states[k], actions[act], next_states[j])*(env.get_reward(states[k], actions[act], next_states[j]) + v_kplus1[states.index(next_states[j])]) + sum\n",
        "        if(sum>max):\n",
        "          max = sum\n",
        "          max_index = act\n",
        "      action_space_dash.append(actions[max_index])\n",
        "    else:\n",
        "      action_space_dash.append('None')\n",
        "  num = 0\n",
        "  for i in range(len(states)):\n",
        "    if(action_space[i]==action_space_dash[i]):\n",
        "      num +=1\n",
        "  if(num==12):\n",
        "    flag1 = 0\n",
        "  action_space = action_space_dash\n",
        "print('Number of steps required in order to obtain the optimal policy and value function is', num_steps)\n",
        "print('Optimal Policy - ', action_space)\n",
        "print('Optimal Value Function - ', v_kplus1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPht0dOEIraS",
        "outputId": "79a89550-a92f-475a-9b2b-e499b9daf8f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of steps required in order to obtain the optimal policy and value function is 7\n",
            "Optimal Policy -  ['a2', 'a2', 'a2', 'None', 'a1', 'None', 'a0', 'None', 'a1', 'a0', 'a0', 'a3']\n",
            "Optimal Value Function -  [0.9551744405284887, 0.9701896255900637, 0.9833257236758887, 0, 0.9416532885684301, 0, 0.8740149975680315, 0, 0.9261864677278419, 0.9123110204249538, 0.8953842793185613, 0.7087918791639867]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration\n",
        "In value iteration, we compute the optimal state value function by iteratively updating the estimate $\\textbf{v(s)}$. We start with a random value function V(s). At each step, we update it:\n",
        "\n",
        "  $V(s) = \\max_{a} \\sum_{s',r'}p(s', r|s,a)[r+\\gamma V(s')]$\n",
        "\n",
        "Hence, we look-ahead one step and go over all possible actions at each iteration to find the maximum."
      ],
      "metadata": {
        "id": "lT0OJO1UoFL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The update step is very similar to the update step in the policy iteration algorithm. The only difference is that we take the maximum over all possible actions in the value iteration algorithm.\n",
        "\n",
        "Instead of evaluating and then improving, the value iteration algorithm updates the state value function in a single step. This is possible by calculating all possible rewards by looking ahead."
      ],
      "metadata": {
        "id": "x_6VhxBGDtfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v_k = []\n",
        "for i in range(len(states)):\n",
        "  v_k.append(0)\n",
        "flag = 1\n",
        "num = 0\n",
        "while(flag==1):\n",
        "  num += 1\n",
        "  v_kplus1 = []\n",
        "  for i in range(len(states)):\n",
        "    actions = env.get_possible_actions(states[i])\n",
        "    if(len(actions)!=0):\n",
        "      max = -10\n",
        "      for j in range(len(actions)):\n",
        "        next_states = list(env.get_next_states(states[i], actions[j]).keys())\n",
        "        sum = 0\n",
        "        for k in range(len(next_states)):\n",
        "          sum = env.get_transition_prob(states[i], actions[j], next_states[k])*(env.get_reward(states[i], actions[j], next_states[k]) + v_k[states.index(next_states[k])]) + sum\n",
        "        if(sum>=max):\n",
        "          max = sum\n",
        "      v_kplus1.append(max)\n",
        "    else:\n",
        "      v_kplus1.append(0)\n",
        "  max = -1\n",
        "  for k in range(len(states)):\n",
        "    if(abs(v_kplus1[k]-v_k[k])>max):\n",
        "      max = abs(v_kplus1[k]-v_k[k])\n",
        "  if(max<=0.01):\n",
        "    flag = 0\n",
        "  v_k = v_kplus1\n",
        "\n",
        "action_space = []\n",
        "for i in range(len(states)):\n",
        "  actions = env.get_possible_actions(states[i])\n",
        "  if(len(actions)!=0):\n",
        "    max = -10\n",
        "    for j in range(len(actions)):\n",
        "      next_states = list(env.get_next_states(states[i], actions[j]).keys())\n",
        "      sum = 0\n",
        "      for k in range(len(next_states)):\n",
        "        sum = env.get_transition_prob(states[i], actions[j], next_states[k])*(env.get_reward(states[i], actions[j], next_states[k]) + v_k[states.index(next_states[k])]) + sum\n",
        "      if(sum>=max):\n",
        "        max = sum\n",
        "        act = actions[j]\n",
        "    action_space.append(act)\n",
        "  else:\n",
        "    action_space.append('None')\n",
        "print('Number of steps required in order to obtain the optimal policy and value function is', num)\n",
        "print('Optimal Policy - ', action_space)\n",
        "print('Optimal Value Function - ', v_kplus1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK_DP-5_oKC4",
        "outputId": "b9115538-4af7-40e4-e389-38c04d3fc766"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of steps required in order to obtain the optimal policy and value function is 20\n",
            "Optimal Policy -  ['a2', 'a2', 'a2', 'None', 'a1', 'None', 'a0', 'None', 'a1', 'a0', 'a0', 'a3']\n",
            "Optimal Value Function -  [0.9528669451625345, 0.9683343099109085, 0.981806266398715, 0, 0.9389091676094532, 0, 0.8625371741259842, 0, 0.9229562493380312, 0.9086724176404589, 0.8903134406019679, 0.7051207359650054]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison between Policy Iteration and Value Iteration in terms of Convergence"
      ],
      "metadata": {
        "id": "g3J2qLCfl2zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the dynamic programming based algorithms are guaranteed to converage to an optimal policy, which for our environment comes out to be the **same** for both algorithms.\n",
        "Policy iteration method converges in a fewer number of iterations(4-5) of the outer loop although each outer loop requires an inner loop of value evaluation untill convergence with an error of 1 percent while value iteration method takes significantly higher steps(around 20) to converge compared to policy iteration.\n",
        "But policy iteration takes more time compared to value iteration for our MDP problem as there is an additional value evaluation step involved in policy iteration but as our action space is limited to 4 actions for every state value iteration takes less time. For more complicated action spaces it might take longer to converge compared to policy iteration."
      ],
      "metadata": {
        "id": "DZZzxCfimNfn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}